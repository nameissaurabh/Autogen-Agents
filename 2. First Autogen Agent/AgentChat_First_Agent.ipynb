{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to AgentChat \n",
    "We’ll use the AgentChat API to create a simple `AssistantAgent` and explore its capabilities.  \n",
    "`AssistantAgent` is a built-in agent that uses a language model and has the ability to use tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.messages import UserMessage\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the google model client\n",
    "model_client = OpenAIChatCompletionClient(model = 'gemini-2.0-flash',\n",
    "                                          api_key = api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'autogen_core.models._types.CreateResult'>\n",
      "finish_reason='stop' content='The capital of France is **Paris**.\\n' usage=RequestUsage(prompt_tokens=7, completion_tokens=9) cached=False logprobs=None thought=None\n",
      "The capital of France is **Paris**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's test the model client \n",
    "response = await model_client.create([UserMessage(content=\"What is the capital of France?\", \n",
    "                                                  source=\"user\")])\n",
    "print(type(response)) \n",
    "print(response) # This will print the response object\n",
    "print(response.content) # This will print the content of the response\n",
    "await model_client.close() # Close the model client connection\n",
    "# Note: The model client should be closed after use to free up resources (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating First AssistantAgent  \n",
    "The `AssistantAgent` is a versatile agent for conversations, powered by AgentChat.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the AssistantAgent\n",
    "model_client = OpenAIChatCompletionClient(model = 'gemini-2.0-flash',\n",
    "                                          api_key = api_key)\n",
    "# Creating First AssistantAgent\n",
    "assistant = AssistantAgent(name = \"assistant\",\n",
    "                           model_client = model_client, \n",
    "                           description = 'Assistant Agent for general conversation',\n",
    "                           system_message = \"You are a helpful assistant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the AssistantAgent  \n",
    "We’ll use the `run` method to send a task and get a response.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[TextMessage(id='1c63b7d5-2da2-45b5-b297-2c260a004788', source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 8, 4, 16, 10, 21, 148981, tzinfo=datetime.timezone.utc), content=\"What's the capital of USA & write 1 line about the same?\", type='TextMessage'), TextMessage(id='afb336e6-060c-417f-b0ff-aade57f5d928', source='assistant', models_usage=RequestUsage(prompt_tokens=22, completion_tokens=27), metadata={}, created_at=datetime.datetime(2025, 8, 4, 16, 10, 22, 478407, tzinfo=datetime.timezone.utc), content=\"The capital of the USA is Washington, D.C. It's where the U.S. federal government is located.\\n\", type='TextMessage')] stop_reason=None\n"
     ]
    }
   ],
   "source": [
    "result = await assistant.run(task=\"What's the capital of USA & write 1 line about the same?\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "messages=[TextMessage(id='1c63b7d5-2da2-45b5-b297-2c260a004788', source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 8, 4, 16, 10, 21, 148981, tzinfo=datetime.timezone.utc), \n",
    "\n",
    "content=\"What's the capital of USA & write 1 line about the same?\", type='TextMessage'), \n",
    "\n",
    "TextMessage(id='afb336e6-060c-417f-b0ff-aade57f5d928', source='assistant', \n",
    "\n",
    "models_usage=RequestUsage(prompt_tokens=22, completion_tokens=27), metadata={}, \n",
    "\n",
    "created_at=datetime.datetime(2025, 8, 4, 16, 10, 22, 478407, tzinfo=datetime.timezone.utc), \n",
    "\n",
    "content=\"The capital of the USA is Washington, D.C. It's where the U.S. federal government is located.\\n\", type='TextMessage')] stop_reason=None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextMessage(id='1c63b7d5-2da2-45b5-b297-2c260a004788', source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 8, 4, 16, 10, 21, 148981, tzinfo=datetime.timezone.utc), content=\"What's the capital of USA & write 1 line about the same?\", type='TextMessage'), TextMessage(id='afb336e6-060c-417f-b0ff-aade57f5d928', source='assistant', models_usage=RequestUsage(prompt_tokens=22, completion_tokens=27), metadata={}, created_at=datetime.datetime(2025, 8, 4, 16, 10, 22, 478407, tzinfo=datetime.timezone.utc), content=\"The capital of the USA is Washington, D.C. It's where the U.S. federal government is located.\\n\", type='TextMessage')]\n"
     ]
    }
   ],
   "source": [
    "print(result.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the capital of USA & write 1 line about the same?\n"
     ]
    }
   ],
   "source": [
    "print(result.messages[0].content)  # This will print the content of the first response\n",
    "# Note: The messages attribute contains the conversation history, including the response from the assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of the USA is Washington, D.C. It's where the U.S. federal government is located.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result.messages[-1].content) # This will print the content of the last response\n",
    "# Note: The messages attribute contains the conversation history, including the response from the assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Messages\n",
    "We can also stream each message as it is generated by the agent by using the `run_stream()` method, and use Console to print the messages as they appear to the `console`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "write something in 2-3 lines on AutoGen!\n",
      "---------- TextMessage (assistant) ----------\n",
      "AutoGen is a framework developed by Microsoft that enables building AI applications with multiple agents that can converse with each other to solve tasks. These agents can be customized with different roles, skills, and access to tools, fostering collaborative problem-solving in a dynamic and automated manner. It simplifies the creation of complex AI workflows by orchestrating interactions between different specialized AI agents.\n",
      "\n",
      "[Prompt tokens: 61, Completion tokens: 73]\n",
      "---------- Summary ----------\n",
      "Number of messages: 2\n",
      "Finish reason: None\n",
      "Total prompt tokens: 61\n",
      "Total completion tokens: 73\n",
      "Duration: 1.69 seconds\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "async def assistant_run_stream() -> None:\n",
    "    # Option 1: read each message from the stream (as shown in the previous example).\n",
    "    # async for message in agent.run_stream(task=\"Find information on AutoGen\"):\n",
    "    #     print(message)\n",
    "\n",
    "    # Option 2: use Console to print all messages as they appear.\n",
    "    await Console(\n",
    "        assistant.run_stream(task=\"write something in 2-3 lines on AutoGen!\"),\n",
    "        output_stats=True,  # Enable stats printing.\n",
    "    )\n",
    "\n",
    "\n",
    "# Use asyncio.run(assistant_run_stream()) when running in a script.\n",
    "await assistant_run_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration with ollama\n",
    "# first pip install below packages\n",
    "#pip install -U \"autogen-ext[ollama]\"\n",
    "# from autogen_core.models import UserMessage\n",
    "# from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
    "\n",
    "# # Assuming your Ollama server is running locally on port 11434.\n",
    "# ollama_model_client = OllamaChatCompletionClient(model=\"llama3.2\")\n",
    "\n",
    "# response = await ollama_model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n",
    "# print(response)\n",
    "# await ollama_model_client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
